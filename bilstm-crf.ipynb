{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eedcab4",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(t, dim):\n",
    "    # t: nD tensor\n",
    "    # dim: dim to sum along\n",
    "    # ret: nD tensor\n",
    "    # log is natural logrithm.\n",
    "    # Note: Will keep dim.\n",
    "    \n",
    "    max_val, _ = torch.max(t,dim=dim,keepdim=True)  # nD tensor\n",
    "    return torch.log(torch.sum(torch.exp(t - max_val), dim=dim, keepdim=True)) + max_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e65f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for log_sum_exp()\n",
    "t = torch.tensor([[1,1,1,1],[1,1,1,1]],dtype=torch.float64)\n",
    "# log_sum_exp(t,1)\n",
    "log_sum_exp(t,0)\n",
    "# Check ln(4e) == 2.3863\n",
    "# Check ln(2e) == 1.6931"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24452e6",
   "metadata": {},
   "source": [
    "# BiLSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L: Length of input sequence\n",
    "# B: Batch size\n",
    "# T: Tag set size\n",
    "# E: Embedding dim\n",
    "# H: Hidden dim\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
    "        \n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim   # E\n",
    "        self.hidden_dim = hidden_dim         # H\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "        \n",
    "        self.START_TAG_IDX = tagset_size - 2  # the second to last is start tag\n",
    "        self.STOP_TAG_IDX = tagset_size - 1   # the last is stop tag\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, \n",
    "                            self.hidden_dim // 2,  # This is bidirectional. Each direction gets H//2\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "        \n",
    "        # entry [i,j] is for transition from i to j\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        self.transitions.data[:, self.START_TAG_IDX] = -10000  # Can't go back to START.  Access .data directly to avoid autograd ?\n",
    "        self.transitions.data[self.STOP_TAG_IDX, :] = -10000   # Can't leave STOP\n",
    " \n",
    "    \n",
    "    def _rand_lstm_hidden(self, bsize):\n",
    "        # ret: (h, c)\n",
    "        # h: hidden state. shape of (layers_num, batch_size, output_size)\n",
    "        # c: cell state. shape of (layers_num, batch_size, output_size)\n",
    "        return (torch.randn(2,bsize,self.hidden_dim//2), torch.randn(2,bsize,self.hidden_dim//2))\n",
    "    \n",
    "    def _lstm_features(self, words_input):\n",
    "        # words_input: words indices in shape of [L, B]\n",
    "        # ret: Shape of [L, B, T]\n",
    "        B = words_input.shape[1]\n",
    "        hidden_state = self._rand_lstm_hidden(B)\n",
    "        embeddings = self.word_embeddings(words_input)  # [L,B,E]\n",
    "        lstm_output, hidden_state = self.lstm(embeddings, hidden_state)\n",
    "        lstm_feat = self.hidden2tag(lstm_output)   # [L,B,H]->[L,B,T]\n",
    "        return lstm_feat\n",
    "    \n",
    "    def _crf_forward(self, features):\n",
    "        # To compute partition function, i.e., sum up all potentials across every possible tag sequence.\n",
    "        # features: BiLSTM output in shape of [L,B,T]\n",
    "        # ret: [B]\n",
    "        B = features.shape[1]\n",
    "        fwd_vars = torch.full((B, self.tagset_size, 1), -10000.)  # Shape of [B, T, 1]\n",
    "        fwd_vars[:, self.START_TAG_IDX, 0] = 0.         # START gets all.\n",
    "        for feat in features:\n",
    "            # feat in shape of [B,T]\n",
    "            # [B,T,1] + [1,T,T] + [B,1,T] = [B,T,T]\n",
    "            next_vars = fwd_vars + self.transitions.unsqueeze(dim=0) + feat.unsqueeze(dim=1)\n",
    "            next_vars = log_sum_exp(next_vars, dim=1)     # [B,1,T]\n",
    "            fwd_vars = torch.transpose(next_vars, 1, 2)   # [B,T,1]\n",
    "        \n",
    "        # [B,T,1] + [1,T,1] = [B,T,1]\n",
    "        terminal_vars = fwd_vars + self.transitions[:, self.STOP_TAG_IDX].unsqueeze(dim=-1).unsqueeze(dim=0)\n",
    "        return log_sum_exp(terminal_vars, dim=1).squeeze(dim=2).squeeze(dim=1)  # [B,T,1]->[B,T]->[B]\n",
    "    \n",
    "    def _crf_decode(self, features):\n",
    "        # features: [L,B,T]\n",
    "        B = features.shape[1]\n",
    "        fwd_vars = torch.full((B, self.tagset_size, 1), -10000.)   # [B,T,1]\n",
    "        fwd_vars[:, self.START_TAG_IDX, 0] = 0.\n",
    "        \n",
    "        backptrs = []\n",
    "        for feat in features:\n",
    "            # feat: [B,T]\n",
    "            # [B,T,1] + [1,T,T] = [B,T,T]\n",
    "            next_vars = fwd_vars + self.transitions.unsqueeze(dim=0)  # No need add emission here, since emission are the same for the same tag.\n",
    "            val, idx = torch.max(next_vars, dim=1)  # [B,T]\n",
    "            backptrs += [idx]\n",
    "            \n",
    "            val += feat                  # Add back emission [B,T]\n",
    "            fwd_vars = val.unsqueeze(2)  # [B,T,1]\n",
    "        \n",
    "        \n",
    "        terminal_vars = fwd_vars.squeeze(2) + self.transitions[:, self.STOP_TAG_IDX]  # [B,T]\n",
    "        best_scores, best_idx = torch.max(terminal_vars, dim=1)   # [B]\n",
    "        \n",
    "        best_paths = [best_idx]\n",
    "        for ptrs in reversed(backptrs):\n",
    "            # ptrs in shape of [B,T]\n",
    "            best_idx = ptrs[range(B), best_idx] # [B]\n",
    "            best_paths += [best_idx]\n",
    "            \n",
    "        start_idx = best_paths.pop()\n",
    "        \n",
    "        assert start_idx.tolist() == [self.START_TAG_IDX] * B\n",
    "        best_paths.reverse()\n",
    "        return best_scores, best_paths\n",
    "        \n",
    "    def _compute_log_potentials(self, features, tags):\n",
    "        # Compute potentials, including transition and emission, in log-space.\n",
    "        # features: lstm features for each word in shape of [L,B,T]\n",
    "        # tags: tag indices for each word in shape of [L,B]\n",
    "        # ret: scores in shape [B]\n",
    "        L, B = tags.shape\n",
    " \n",
    "        pre_tags = torch.cat([torch.full((1,B), self.START_TAG_IDX), tags], dim=0)  # (L+1,B)\n",
    "        next_tags = torch.cat([tags, torch.full((1,B), self.STOP_TAG_IDX)], dim=0)  # (L+1,B)\n",
    "        \n",
    "        tran_scores = self.transitions[pre_tags.view(-1), next_tags.view(-1)].view(L+1, B)  # (L+1,B)\n",
    "        tran_sum = torch.sum(tran_scores, dim=0) # [B]\n",
    "        \n",
    "        emis_scores = features.view(L*B, self.tagset_size)[range(L*B), tags.view(-1)].view(L,B) # [L,B]\n",
    "        emis_sum = torch.sum(emis_scores, dim=0) # [B]\n",
    "        return tran_sum + emis_sum\n",
    "\n",
    "    def neg_log_likelihood(self, words_input, tags):\n",
    "        # words_input: [L,B,E]\n",
    "        # tags: [L,B]\n",
    "        # ret:  loss as a scalar\n",
    "        lstm_feat = self._lstm_features(words_input)\n",
    "        partition_term = self._crf_forward(lstm_feat)\n",
    "        potentials = self._compute_log_potentials(lstm_feat, tags)\n",
    "        \n",
    "        return torch.mean(partition_term - potentials)\n",
    "    \n",
    "    def forward(self, words_input):\n",
    "        # words_input: words indices in shape of [L,B]\n",
    "        lstm_feats = self._lstm_features(words_input)\n",
    "        score, tag_seq = self._crf_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c26c3c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26b54b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    "),\n",
    "(\n",
    "    \"I worked for Shopee an eCommerce company in Singapore as an Engineer\".split(),\n",
    "    \"O O O B O O O O B O O O\".split()\n",
    "),\n",
    "(\n",
    "    \"Yichang is famous for Three Gorges and the dam\".split(),\n",
    "    \"B O O O B I O B I\".split()\n",
    "),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        # dataset: a list of tuples of (tokens, tags)\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.word2idx = {}\n",
    "        self.tag2idx = {}\n",
    "        for sentence, tags in self.dataset:\n",
    "            for word in sentence:\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = len(self.word2idx)\n",
    "            for tag in tags:\n",
    "                if tag not in self.tag2idx:\n",
    "                    self.tag2idx[tag] = len(self.tag2idx)\n",
    "                \n",
    "        self.word2idx['<pad>'] = len(self.word2idx)\n",
    "        self.tag2idx['<START>'] = len(self.tag2idx)  # the second to last is start tag\n",
    "        self.tag2idx['<STOP>'] = len(self.tag2idx)   # the last is stop tag\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.dataset[idx][0]\n",
    "        tags = self.dataset[idx][1]\n",
    "        words_indices = [self.word2idx[w] for w in tokens]\n",
    "        tags_indices = [self.tag2idx[t] for t in tags]\n",
    "        \n",
    "        return words_indices, tags_indices\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def word2idx(self, word):\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def tag2idx(self, tag):\n",
    "        return self.tag2idx[tag]\n",
    "    \n",
    "    def tagset_size(self):\n",
    "        return len(self.tag2idx)\n",
    "    \n",
    "    def prepare_sequence(self, sent):\n",
    "        # sent: a list of tokens\n",
    "        # ret: a 2d tensor for words indices\n",
    "        idxs = [[self.word2idx[w] for w in sent]]\n",
    "        return torch.tensor(idxs, dtype=torch.long).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NERDataset(training_data)\n",
    "\n",
    "#WARNNING collate_fn depends on a dataset.\n",
    "def nerdataset_collate(batch):\n",
    "    # batch: a list of tuples of (tokens, tags)\n",
    "    # ret: a tensor for words indices and a tensor for tag indices\n",
    "    max_len = max([len(tokens) for tokens, _ in batch])\n",
    "    batched_tokens = []\n",
    "    batched_tags = []\n",
    "    for tokens, tags in batch:\n",
    "        tokens += [dataset.word2idx['<pad>']] * (max_len - len(tokens))\n",
    "        tags += [dataset.tag2idx['O']] * (max_len - len(tags))\n",
    "        \n",
    "        batched_tokens +=[tokens]  # [B,L]\n",
    "        batched_tags += [tags]     # [B,L]\n",
    "    \n",
    "    return torch.tensor(batched_tokens, dtype=torch.int64).T.contiguous(), torch.tensor(batched_tags, dtype=torch.int64).T.contiguous()\n",
    " \n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=nerdataset_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9f928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be91bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(dataset.vocab_size(), dataset.tagset_size(), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "with torch.no_grad():\n",
    "    precheck_sent = dataset.prepare_sequence(training_data[0][0])\n",
    "    precheck_tags = torch.tensor([[dataset.tag2idx[t] for t in training_data[0][1]]], dtype=torch.long).T\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ee8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for tokens, tags in loader: \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(tokens, tags)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = dataset.prepare_sequence(training_data[0][0])\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade916fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
